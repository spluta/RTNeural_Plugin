#N canvas 642 36 636 998 12;
#N canvas 722 36 1021 770 mlp_control_training 0;
#X msg 494 151 set_learn_rate 0.001;
#X msg 494 69 clear_points;
#X msg 494 126 set_epochs 5000;
#X msg 494 264 bypass 1;
#X obj 494 359 array get \$0-array2;
#X listbox 494 384 20 0 0 0 - - - 0;
#X obj 494 324 bng 30 250 50 0 empty empty empty 0 -10 0 12 #dfdfdf #000000 #000000;
#X listbox 739 384 20 0 0 0 - - - 0;
#X obj 739 433 list trim;
#X obj 739 359 array get \$0-array10;
#X obj 739 408 list prepend add_output;
#X obj 494 408 list prepend add_input;
#X obj 494 433 list trim;
#X text 780 321 add an output set from array10, f 17;
#X msg 494 473 remove_point 3;
#X text 473 510 5) write the configuration to a json file;
#X msg 165 291 post_points;
#X text 164 253 print stored point data to Pd console, f 18;
#X obj 66 352 rtneural 2 10;
#X obj 616 69 s \$0-to_rtneural;
#X obj 472 201 s \$0-to_rtneural;
#X obj 588 433 s \$0-to_rtneural;
#X obj 494 559 s \$0-to_rtneural;
#X obj 66 377 array set \$0-array10;
#N canvas 0 50 450 250 (subpatch) 0;
#X array \$0-array10 10 float 2;
#A color 0;
#A width 2;
#X coords 0 1 10 0 400 200 1 0 0;
#X restore 40 433 graph;
#N canvas 0 50 450 250 (subpatch) 0;
#X array \$0-array2 2 float 2;
#A color 0;
#A width 2;
#X coords 0 1 2 0 40 200 1 0 0;
#X restore 40 96 graph;
#X obj 591 264 s \$0-to_rtneural;
#X text 473 300 4) configure input and output data points;
#X text 473 240 3) bypass the model;
#X text 473 102 2) set the the training data variables: epochs \, learn_rate \, layers_data, f 72;
#X text 473 45 1) clear the internal data points;
#X text 552 321 add an input set from array2, f 16;
#X text 416 20 [rtneural] can also be used to make a new training:;
#X obj 66 327 r \$0-to_rtneural;
#X text 611 473 remove the set at index 3 of both input and output;
#X text 473 599 6) finally \, train the configured model via the command line as described in RTNeural_python/MLP_control/README.md (NOTE: this may take a while...), f 75;
#X msg 494 176 set_layers_data 20 relu 20 relu 10 sigmoid;
#X msg 494 534 write_json saved_models/mlp/mlp_training.json;
#X text 485 648 from the RTNeural_python directory in the terminal: 1) make sure you are in a virtual environment as described by RTNeural_python/README.md
2) run 'python MLP_control/mlp_control_train_convert.py -f <your file>' to train the network the file will be saved as <your file without the extension>_RTNeural.json in the same directory as <your file>, f 71;
#X connect 0 0 20 0;
#X connect 1 0 19 0;
#X connect 2 0 20 0;
#X connect 3 0 26 0;
#X connect 4 0 5 0;
#X connect 5 0 11 0;
#X connect 6 0 4 0;
#X connect 6 0 9 0;
#X connect 7 0 10 0;
#X connect 8 0 21 0;
#X connect 9 0 7 0;
#X connect 10 0 8 0;
#X connect 11 0 12 0;
#X connect 12 0 21 0;
#X connect 14 0 21 0;
#X connect 16 0 18 0;
#X connect 18 0 23 0;
#X connect 33 0 18 0;
#X connect 36 0 20 0;
#X connect 37 0 22 0;
#X restore 21 129 pd mlp_control_training;
#N canvas 722 38 885 396 mlp_control 0;
#X obj 397 20 vsl 19 162 0 1 0 0 empty empty empty 0 -9 0 12 #dfdfdf #000000 #000000 0 1;
#X obj 422 20 vsl 19 162 0 1 0 0 empty empty empty 0 -9 0 12 #dfdfdf #000000 #000000 0 1;
#X obj 412 215 pack;
#X obj 412 190 t b f;
#X obj 227 293 rtneural 2 10;
#X obj 429 296 tgl 30 0 empty empty empty 0 -10 0 12 #dfdfdf #000000 #000000 0 1;
#X msg 429 355 bypass \$1;
#N canvas 0 0 450 300 (subpatch) 0;
#X array \$0-neural_out 10 float 2;
#A color 0;
#A width 2;
#X coords 0 1 10 0 300 150 1 0 0;
#X restore 27 46 graph;
#X text 471 191 Load the model file at the given (relative or absolute) path. The model must satisfy [rtneural]'s channel counts.;
#X text 21 208 [rtneural] runs inference from a pre-trained neural net on an input list. It accepts two creation arguments: input list length and output list length., f 44;
#X text 471 290 set whether [rtneural]'s processing is bypassed (if set to 1 \, no output values are generated when input values are received), f 52;
#X floatatom 429 331 5 0 0 0 - - - 0;
#X obj 227 318 array set \$0-neural_out;
#X text 470 56 [rtneural] expects an input list of the declared length. The values are controlled by these faders., f 50;
#X msg 472 231 load_model saved_models/mlp/mlp_training_RTNeural.json;
#X connect 0 0 2 0;
#X connect 1 0 3 0;
#X connect 2 0 4 0;
#X connect 3 0 2 0;
#X connect 3 1 2 1;
#X connect 4 0 12 0;
#X connect 5 0 11 0;
#X connect 6 0 4 0;
#X connect 11 0 6 0;
#X connect 14 0 4 0;
#X restore 21 96 pd mlp_control;
#X text 195 95 <-- multi layer perceptron inference;
#X text 195 129 <-- learn how to make a new mlp training;
#X text 195 163 <-- note prediction using an lstm model;
#N canvas 367 74 1189 931 note_prediction 1;
#X obj 77 376 loadbang;
#X msg 77 401 load_model saved_models/lstm_note_prediction/bach_RTNeural.json;
#X obj 77 441 rtneural 1 84;
#X obj 230 136 bng 30 50 50 0 empty empty (re)init\ with\ n 0 -10 0 12 #c6ffc7 #000000 #000000;
#N canvas 735 410 450 250 (subpatch) 0;
#X array \$0-notep-pitch-softmax 84 float 2;
#A color 0;
#A width 2;
#X coords 0 1 84 -1 200 140 1 0 0;
#X restore 196 775 graph;
#X floatatom 230 231 5 1 146 0 n - - 16;
#X obj 37 605 loadbang;
#X msg 584 660 16 16 2 2 8 4 8 4 4 8 25 2 2 12 2 2 2 2 2 2 2 2 1 1 1 1 1 1 4 4 2 2 4 2 2 4 8 25 16 16 2 2 8 4 8 4 4 8 16 2 2 2 2 2 6 2 6 2 6 2 6 4 8 4 2 10 4 8 2 2 21 16 16 2 2 8 4 4 4 4 34 1 1 1 16 2 2 2 2 2 2 2 2 1 1 1 1 1 1 4 4 8 4 4 2 2 2 2 2 2 1 1 1 8 8 4 4 4 4 8 18 2 4 4 4 4 4 8 4 14 2 4 4 4 4 4 8 16 8 4 6 2 2 2 8 16;
#X msg 37 630 79 79 81 79 81 83 81 79 78 76 74 67 66 67 69 67 66 67 69 67 69 67 69 67 69 67 69 67 66 67 69 67 66 67 66 64 64 62 74 74 76 74 76 77 76 74 72 71 69 79 78 76 78 79 78 81 79 78 76 74 72 72 81 72 71 67 66 66 67 66 67 71 71 73 71 73 74 74 73 71 69 67 71 76 79 81 79 78 79 81 79 81 79 81 79 81 79 81 79 78 79 79 78 76 74 73 71 73 74 73 74 73 74 73 76 81 79 78 76 74 69 71 72 71 69 67 66 64 74 73 74 76 74 73 71 69 79 71 73 73 74 76 74 73 73 74;
#X obj 584 635 loadbang;
#X msg 230 259 all \$1;
#X obj 162 171 loadbang;
#X obj 162 196 f 1;
#X obj 942 237 route 0 1;
#X obj 631 376 loadbang;
#X msg 631 401 load_model saved_models/lstm_note_prediction/bach_dur_RTNeural.json;
#X obj 631 441 rtneural 1 35;
#X text 740 444 the greatest duration in this series is 34 \, so the network must have a size of 35, f 43;
#X text 184 441 the highest note in this series is 83 \, so the network must have a size of 84, f 38;
#N canvas 0 50 450 250 (subpatch) 0;
#X array \$0-notep-dur-softmax 35 float 2;
#A color 0;
#A width 2;
#X coords 0 1 35 -1 200 140 1 0 0;
#X restore 733 775 graph;
#X obj 951 177 bng 30 50 50 0 empty empty next 0 -10 0 12 #c6ffc7 #000000 #000000;
#X obj 777 136 hradio 30 1 0 2 empty empty empty 0 -10 0 12 #c6ffc7 #000000 #000000 0;
#X msg 777 171 all \$1;
#X obj 951 212 t b b;
#X msg 1037 212 reset;
#X obj 37 760 s \$0-notep-pitch-l;
#X obj 584 760 s \$0-notep-dur-l;
#X obj 928 287 s \$0-notep-pitch-rtn;
#X obj 972 312 s \$0-notep-dur-rtn;
#X obj 47 351 r \$0-notep-pitch-rtn;
#X obj 601 351 r \$0-notep-dur-rtn;
#X obj 680 201 r \$0-notep-pitch-l;
#X obj 793 226 r \$0-notep-dur-l;
#X obj 1027 237 s \$0-notep-pitch-rtn;
#X obj 1037 262 s \$0-notep-dur-rtn;
#X msg 289 171 reset;
#X obj 279 196 s \$0-notep-pitch-rtn;
#X obj 289 221 s \$0-notep-dur-rtn;
#X obj 230 171 t b b;
#X obj 269 296 clone rtneural-help-notep-queue 2;
#X obj 52 516 list;
#X obj 606 516 list;
#X obj 680 226 list prepend 0;
#X obj 793 251 list prepend 1;
#X obj 77 466 list prepend 0;
#X obj 631 466 list prepend 1;
#X obj 353 466 mtof;
#X obj 353 491 phasor~;
#X obj 353 541 output~;
#X obj 269 321 route 0 1;
#X obj 394 346 delay;
#X obj 269 346 pack;
#X obj 52 491 r \$0-notep-pitch-next;
#X obj 606 491 r \$0-notep-dur-next;
#X obj 941 337 s \$0-notep-pitch-next;
#X obj 959 362 s \$0-notep-dur-next;
#X obj 438 356 spigot;
#X obj 504 356 tgl 30 0 empty empty play/pause 0 -10 0 12 #c6ffc7 #000000 #000000 0 1;
#X obj 394 321 * 50;
#X obj 30 37 tgl 30 0 empty empty DSP\ on/off 0 -10 0 12 #ffc7c6 #000000 #000000 0 1;
#X obj 30 72 switch~;
#X obj 353 516 *~ 0.8;
#X obj 504 391 route 1;
#X text 188 232 -->;
#X text 120 23 this patch is bit more complicated. it uses two data sequences from a bach suite \, note pitches and note durations \, on which two [rtneural] interfaces are trained to predict the next pitch/duration. on (re)init \, a random slice of n values is initialized as a queue for each sequence \, from which each [rtneural] generate softmaxes to predict the next pitch/duration \, and a [softmax] object selects a value from the prediction (see the 'rtneural-help-notep-queue' subpatch). on -next- (or automatically with -play/pause- enabled) \, this prediction is pushed to the start of the queue \, and the process is repeated., f 130;
#X obj 269 371 print note;
#X text 451 128 in the subpatch \, [softmax] will select either the best value (left cell) or a random value (right cell) based off the softmax's weights. best value tends to get stuck in cycles., f 45;
#X obj 17 551 array set \$0-notep-pitch-softmax;
#X obj 571 551 array set \$0-notep-dur-softmax;
#X connect 0 0 1 0;
#X connect 1 0 2 0;
#X connect 2 0 44 0;
#X connect 2 0 67 0;
#X connect 3 0 38 0;
#X connect 5 0 10 0;
#X connect 6 0 8 0;
#X connect 7 0 26 0;
#X connect 8 0 25 0;
#X connect 9 0 7 0;
#X connect 10 0 39 1;
#X connect 11 0 12 0;
#X connect 12 0 5 0;
#X connect 13 0 27 0;
#X connect 13 1 28 0;
#X connect 14 0 15 0;
#X connect 15 0 16 0;
#X connect 16 0 45 0;
#X connect 16 0 68 0;
#X connect 20 0 23 0;
#X connect 21 0 22 0;
#X connect 22 0 39 3;
#X connect 23 0 55 0;
#X connect 23 0 54 0;
#X connect 23 1 24 0;
#X connect 24 0 33 0;
#X connect 24 0 34 0;
#X connect 29 0 2 0;
#X connect 30 0 16 0;
#X connect 31 0 42 0;
#X connect 32 0 43 0;
#X connect 35 0 36 0;
#X connect 35 0 37 0;
#X connect 38 0 5 0;
#X connect 38 1 35 0;
#X connect 39 0 49 0;
#X connect 39 1 13 0;
#X connect 40 0 39 0;
#X connect 41 0 39 0;
#X connect 42 0 39 2;
#X connect 43 0 39 2;
#X connect 44 0 40 1;
#X connect 45 0 41 1;
#X connect 46 0 47 0;
#X connect 47 0 61 0;
#X connect 49 0 51 0;
#X connect 49 0 46 0;
#X connect 49 1 51 1;
#X connect 49 1 58 0;
#X connect 50 0 56 0;
#X connect 51 0 65 0;
#X connect 52 0 40 0;
#X connect 53 0 41 0;
#X connect 56 0 20 0;
#X connect 57 0 56 1;
#X connect 57 0 62 0;
#X connect 58 0 50 0;
#X connect 59 0 60 0;
#X connect 61 0 48 0;
#X connect 62 0 20 0;
#X restore 21 162 pd note_prediction;
#X text 86 17 is a data rate neural inferencing external which uses the RTNeural inference engine to load and run tensorflow and pytorch trained neural network models of any shape or size., f 62;
#X obj 21 17 rtneural;
